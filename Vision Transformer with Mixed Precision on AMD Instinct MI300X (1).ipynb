{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cb34b4-da12-4636-be47-d487fe165928",
   "metadata": {},
   "source": [
    "\n",
    "# Vision Transformer with Mixed Precision on AMD Instinct MI300X 💻⚡\n",
    "\n",
    "Welcome to my personal deep dive into Vision Transformers (ViTs)! I’ve always been intrigued by how Transformers—originally famous for NLP—can excel in computer vision tasks. In this notebook, I’m training a custom ViT on CIFAR-10, using **mixed precision** on an **AMD Instinct MI300X** GPU. 🎉\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Project? 🤔\n",
    "\n",
    "I’ve worked with CNNs for image tasks, but Transformers kept stealing the spotlight—especially since the original \"Attention is All You Need\" paper. I wanted to experience:\n",
    "\n",
    "1. **RandAugment** 🌟: a super-quick trick for stronger data augmentation on CIFAR-10.\n",
    "2. **Mixed Precision** 🚀: to speed up training on AMD hardware (a first for me).\n",
    "3. **Early-Stop at 90%** 🚦: because I’m focusing on iterative improvement rather than blindly training for 200 epochs.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview 📝\n",
    "\n",
    "1. **Data**: [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).  \n",
    "   - 50k training images, 10k test images, 10 classes, each 32×32.\n",
    "2. **Model**:  \n",
    "   - Patch size = 4 (so each “patch” is 4×4),\n",
    "   - Embedding dimension = 384,\n",
    "   - 8 Transformer layers,\n",
    "   - 6 attention heads.\n",
    "3. **Training**:\n",
    "   - Up to 50 epochs,\n",
    "   - Batch size = 128,\n",
    "   - My personal preference: Let’s see if we can get to ~85% accuracy and see how far the final model can push.\n",
    "\n",
    "I love how flexible the ViT architecture is—it’s a perfect example of “attention-based” approaches crossing domains. 🧠💡\n",
    "\n",
    "---\n",
    "\n",
    "## About the AMD Instinct MI300X GPU 🔬\n",
    "\n",
    "I’m running this on AMD Instinct MI300X, which is basically the HPC-oriented GPU from AMD. It's awesome for large-batch training because it has a ton of memory. Also:\n",
    "\n",
    "- MI300X has advanced **ROC**m and BF16/FP16 training performance.\n",
    "- I’m using PyTorch’s built-in support for ROCm.\n",
    "\n",
    "*(If you’re on a different AMD GPU or HPC environment, you can adapt the code accordingly.)*\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Dependencies 🏗️\n",
    "\n",
    "- **PyTorch** with ROCm 6.1 support (or whichever version you use).\n",
    "- Basic Python libraries: `torchvision`, `tqdm`, `numpy`, `matplotlib`, etc.\n",
    "\n",
    "Example installation snippet (though you may have your environment pre-configured):\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm6.1\n",
    "pip install tqdm matplotlib pandas\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Sections 📔\n",
    "\n",
    "1. **Imports & Hyperparameters**  \n",
    "   I declare all the key settings: patch size, embed dimension, batch size, epochs, etc.  \n",
    "2. **Data Loaders**  \n",
    "   - CIFAR-10 with RandAugment (2 ops, magnitude=9).  \n",
    "   - My reason: RandAugment is quick to set up and can significantly boost accuracy on smaller datasets.\n",
    "3. **Vision Transformer Implementation**  \n",
    "   - `PatchEmbedding` for 32×32 → flatten → embed.  \n",
    "   - Then standard MLP, multi-head attention blocks, etc.  \n",
    "4. **Training Loop**  \n",
    "   - Mixed Precision via `autocast(enabled=True)`.  \n",
    "   - Early-stop if test accuracy ≥ 90%.  \n",
    "5. **Inference Benchmark**  \n",
    "   - I do a short warm-up, then measure how many images/sec the model can process at batch sizes 1,4,16,64,128.  \n",
    "   - Because let’s face it, we all love seeing big throughput numbers!  \n",
    "6. **Visualizations**  \n",
    "   - I plot training curves in `training_metrics.png`.  \n",
    "   - I also show random sample predictions in `sample_predictions.png`.\n",
    "\n",
    "---\n",
    "\n",
    "## Let’s Begin! 🎈\n",
    "\n",
    "1. **Check GPU**: I like to do:\n",
    "   ```python\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   print(\"[INFO] Device type:\", device)\n",
    "   if device.type == \"cuda\":\n",
    "       print(\"[INFO] GPU name:\", torch.cuda.get_device_name(0))\n",
    "   ```\n",
    "   Because I want to confirm I’m actually on the MI300X.\n",
    "\n",
    "2. **Data**:  \n",
    "   - `get_cifar10_loaders()`—it’s super simple: once you see “Files already downloaded,” you know you’re ready.\n",
    "\n",
    "*(At this point, code in the following cells sets up transformations and data loaders.)*\n",
    "\n",
    "---\n",
    "\n",
    "## Training 🏋️‍♀️\n",
    "\n",
    "- The code inside `train_one_epoch()` uses `autocast` with standard AMP.  \n",
    "- I personally appreciate how PyTorch’s `GradScaler` helps avoid the usual half-precision pitfalls.\n",
    "\n",
    "You’ll see a progress bar from **`tqdm`**:\n",
    "\n",
    "```\n",
    "Epoch 1/50: 100%|██████████| 391/391 [00:14<00:00, 27.42it/s, loss=1.9474, acc=26.60]\n",
    "[Epoch 1/50] Train Acc=26.60% | Test Acc=39.68%\n",
    "```\n",
    "\n",
    "*(Yes, it’s only 26.60% after the first epoch, but that’s normal for a bigger architecture on CIFAR-10.)*\n",
    "\n",
    "I log final training time around 13 minutes to get to ~84% best test accuracy.  I consider that quite efficient for a mild-scale model on a single GPU. 💪\n",
    "\n",
    "---\n",
    "\n",
    "## Inference & Scaling 🚀\n",
    "\n",
    "- The code in `benchmark_inference()` runs batch sizes `[1,4,16,64,128]`.\n",
    "- I warm up 10 iterations, then measure a certain number (20 or 50) of actual runs.\n",
    "- Output example (which you’ll see in the notebook’s cell):\n",
    "  ```\n",
    "  [Inference] BS=1,  3.47 ms, 288.09 img/s\n",
    "  [Inference] BS=4,  3.74 ms, 1068.95 img/s\n",
    "  ...\n",
    "  [Inference] BS=128, 6.25 ms, 20482.77 img/s\n",
    "  ```\n",
    "\n",
    "To me, these throughput numbers are quite satisfying. 📈💯\n",
    "\n",
    "---\n",
    "\n",
    "## My Personal Reflections 🤗\n",
    "\n",
    "- I love how easy it is to integrate Transformers in PyTorch, especially for smaller images like 32×32.  \n",
    "- Mixed precision definitely speeds up training on the MI300X while being stable.  \n",
    "- Reaching 84% is quite decent for a moderate-sized ViT in ~13 minutes.  \n",
    "- If I wanted to surpass 90%, I’d run more epochs or enlarge the network. But for now, I’m satisfied with these results as a baseline demonstration.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps 🍀\n",
    "\n",
    "1. **Longer Training**: Try 100+ epochs for potential 90%+ accuracy.  \n",
    "2. **Bigger ViT**: Increase `EMBED_DIM` from 384 to 512 or 768.  \n",
    "3. **More Data**: CIFAR-100 or a subset of ImageNet, for a real test.  \n",
    "4. **Advanced Augment**: Could do Mixup or CutMix in synergy with RandAugment.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion ✨\n",
    "\n",
    "I hope walking through this notebook gave insight into:\n",
    "- Setting up a Vision Transformer on AMD hardware,\n",
    "- Achieving real-time speed improvements with AMP,\n",
    "- And seeing a fun look at how Transformers handle small images.\n",
    "\n",
    "**Thanks for checking out my personal notes!**  \n",
    "Feel free to tweak the code or explore further. Let’s keep pushing those attention-based architectures to new frontiers!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27afb6c6-e7ff-48f5-9911-5f09ca0ed66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device type: cuda\n",
      "[INFO] GPU name: AMD Instinct MI300X\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Device type:\", device)\n",
    "if device.type == \"cuda\":\n",
    "    print(\"[INFO] GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b0751d-dc00-40f1-b17a-6dfc726585b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device Type: cuda\n",
      "[INFO] GPU Model: AMD Instinct MI300X\n",
      "[MAIN] Vision Transformer on CIFAR-10 (50 epochs, standard AMP)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[MAIN] #parameters: 14,244,490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 391/391 [00:13<00:00, 28.44it/s, loss=1.9477, acc=26.29]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] Train Acc=26.29% | Test Acc=35.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 391/391 [00:14<00:00, 27.83it/s, loss=1.6824, acc=37.80]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/50] Train Acc=37.80% | Test Acc=42.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 391/391 [00:14<00:00, 26.51it/s, loss=1.5544, acc=42.81]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/50] Train Acc=42.81% | Test Acc=50.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 391/391 [00:14<00:00, 26.51it/s, loss=1.4716, acc=46.34]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/50] Train Acc=46.34% | Test Acc=53.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 391/391 [00:14<00:00, 26.40it/s, loss=1.4076, acc=48.79]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50] Train Acc=48.79% | Test Acc=55.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 391/391 [00:13<00:00, 28.20it/s, loss=1.3493, acc=51.22]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/50] Train Acc=51.22% | Test Acc=58.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 391/391 [00:14<00:00, 27.58it/s, loss=1.2940, acc=53.00]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/50] Train Acc=53.00% | Test Acc=58.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 391/391 [00:15<00:00, 25.10it/s, loss=1.2428, acc=55.35]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/50] Train Acc=55.35% | Test Acc=63.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 391/391 [00:14<00:00, 26.14it/s, loss=1.2025, acc=56.96]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/50] Train Acc=56.96% | Test Acc=63.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 391/391 [00:15<00:00, 24.71it/s, loss=1.1598, acc=58.66]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/50] Train Acc=58.66% | Test Acc=66.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 391/391 [00:14<00:00, 26.21it/s, loss=1.1235, acc=59.67]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/50] Train Acc=59.67% | Test Acc=66.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 391/391 [00:14<00:00, 26.96it/s, loss=1.0877, acc=61.42]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 12/50] Train Acc=61.42% | Test Acc=67.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 391/391 [00:14<00:00, 26.77it/s, loss=1.0617, acc=62.30]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/50] Train Acc=62.30% | Test Acc=68.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 391/391 [00:14<00:00, 26.62it/s, loss=1.0266, acc=63.36]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/50] Train Acc=63.36% | Test Acc=68.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 391/391 [00:14<00:00, 27.82it/s, loss=0.9963, acc=64.91]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/50] Train Acc=64.91% | Test Acc=70.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 391/391 [00:14<00:00, 27.45it/s, loss=0.9715, acc=65.55]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/50] Train Acc=65.55% | Test Acc=71.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 391/391 [00:15<00:00, 25.52it/s, loss=0.9430, acc=66.75]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/50] Train Acc=66.75% | Test Acc=72.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 391/391 [00:14<00:00, 26.61it/s, loss=0.9246, acc=67.21]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 18/50] Train Acc=67.21% | Test Acc=73.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 391/391 [00:14<00:00, 26.14it/s, loss=0.9000, acc=68.28]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/50] Train Acc=68.28% | Test Acc=73.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 391/391 [00:14<00:00, 27.27it/s, loss=0.8879, acc=68.69]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/50] Train Acc=68.69% | Test Acc=74.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 391/391 [00:13<00:00, 28.06it/s, loss=0.8553, acc=69.75]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 21/50] Train Acc=69.75% | Test Acc=75.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 391/391 [00:15<00:00, 25.70it/s, loss=0.8392, acc=70.38]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 22/50] Train Acc=70.38% | Test Acc=76.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 391/391 [00:14<00:00, 26.32it/s, loss=0.8168, acc=71.32]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/50] Train Acc=71.32% | Test Acc=77.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 391/391 [00:14<00:00, 26.95it/s, loss=0.7964, acc=71.82]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 24/50] Train Acc=71.82% | Test Acc=76.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 391/391 [00:14<00:00, 26.94it/s, loss=0.7795, acc=72.49]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/50] Train Acc=72.49% | Test Acc=77.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 391/391 [00:14<00:00, 27.36it/s, loss=0.7663, acc=73.13]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26/50] Train Acc=73.13% | Test Acc=76.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 391/391 [00:14<00:00, 26.89it/s, loss=0.7451, acc=73.53]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 27/50] Train Acc=73.53% | Test Acc=78.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 391/391 [00:15<00:00, 25.37it/s, loss=0.7289, acc=74.22]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 28/50] Train Acc=74.22% | Test Acc=78.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 391/391 [00:14<00:00, 27.68it/s, loss=0.7124, acc=75.02]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 29/50] Train Acc=75.02% | Test Acc=78.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 391/391 [00:14<00:00, 27.08it/s, loss=0.6974, acc=75.36]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 30/50] Train Acc=75.36% | Test Acc=79.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 391/391 [00:14<00:00, 26.54it/s, loss=0.6822, acc=75.77]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 31/50] Train Acc=75.77% | Test Acc=79.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 391/391 [00:14<00:00, 27.65it/s, loss=0.6633, acc=76.52]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 32/50] Train Acc=76.52% | Test Acc=80.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 391/391 [00:14<00:00, 26.16it/s, loss=0.6537, acc=76.85]\n",
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 33/50] Train Acc=76.85% | Test Acc=80.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 391/391 [00:14<00:00, 27.33it/s, loss=0.6368, acc=77.71]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 34/50] Train Acc=77.71% | Test Acc=80.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 391/391 [00:14<00:00, 26.74it/s, loss=0.6237, acc=77.97]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 35/50] Train Acc=77.97% | Test Acc=81.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 391/391 [00:14<00:00, 27.69it/s, loss=0.6123, acc=78.46]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 36/50] Train Acc=78.46% | Test Acc=81.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 391/391 [00:15<00:00, 25.71it/s, loss=0.5970, acc=78.96]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 37/50] Train Acc=78.96% | Test Acc=81.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 391/391 [00:15<00:00, 25.28it/s, loss=0.5889, acc=79.16]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 38/50] Train Acc=79.16% | Test Acc=81.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 391/391 [00:14<00:00, 27.54it/s, loss=0.5734, acc=79.65]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 39/50] Train Acc=79.65% | Test Acc=81.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 391/391 [00:14<00:00, 27.40it/s, loss=0.5672, acc=79.91]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 40/50] Train Acc=79.91% | Test Acc=82.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 391/391 [00:14<00:00, 27.40it/s, loss=0.5558, acc=80.19]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 41/50] Train Acc=80.19% | Test Acc=82.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 391/391 [00:14<00:00, 26.24it/s, loss=0.5471, acc=80.49]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 42/50] Train Acc=80.49% | Test Acc=82.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 391/391 [00:15<00:00, 25.78it/s, loss=0.5423, acc=80.80]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 43/50] Train Acc=80.80% | Test Acc=82.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 391/391 [00:15<00:00, 25.21it/s, loss=0.5374, acc=81.00]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 44/50] Train Acc=81.00% | Test Acc=82.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 391/391 [00:16<00:00, 23.87it/s, loss=0.5259, acc=81.35]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 45/50] Train Acc=81.35% | Test Acc=82.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 391/391 [00:14<00:00, 27.92it/s, loss=0.5288, acc=81.26]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 46/50] Train Acc=81.26% | Test Acc=82.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 391/391 [00:14<00:00, 26.46it/s, loss=0.5216, acc=81.50]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 47/50] Train Acc=81.50% | Test Acc=83.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 391/391 [00:14<00:00, 26.57it/s, loss=0.5212, acc=81.57]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 48/50] Train Acc=81.57% | Test Acc=83.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 391/391 [00:14<00:00, 26.63it/s, loss=0.5154, acc=81.70]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 49/50] Train Acc=81.70% | Test Acc=83.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 391/391 [00:14<00:00, 27.05it/s, loss=0.5159, acc=81.65]\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 50/50] Train Acc=81.65% | Test Acc=82.96%\n",
      "[MAIN] Training took 13.41 minutes. Best test acc=83.08%\n",
      "[MAIN] Model checkpoint saved.\n",
      "[Inference] BS=1, 3.47 ms, 288.10 img/s\n",
      "[Inference] BS=4, 3.74 ms, 1068.96 img/s\n",
      "[Inference] BS=16, 4.33 ms, 3697.96 img/s\n",
      "[Inference] BS=64, 5.29 ms, 12087.71 img/s\n",
      "[Inference] BS=128, 6.25 ms, 20482.77 img/s\n",
      "[INFO] Inference stats saved: results/inference_stats.json\n",
      "[MAIN] All tasks done. Check 'results/' for plots and 'checkpoints/' for model.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MI300X CIFAR-10 Vision Transformer (FP16 via Standard AMP):\n",
    "- 50 epochs (or early-stop at 90% accuracy)\n",
    "- RandAugment for improved accuracy\n",
    "- Mixed / half-precision using autocast(enabled=True)\n",
    "- Prints GPU model name in console output\n",
    "- Plots training metrics with \"MI300X\" explicitly mentioned\n",
    "- Larger sample predictions figure so text isn't cut off\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    RandAugment\n",
    ")\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from datetime import datetime\n",
    "\n",
    "matplotlib.use(\"Agg\")  # For headless environments\n",
    "\n",
    "# -------------------------------\n",
    "# Hyperparameters & Config\n",
    "# -------------------------------\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 50\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "PATCH_SIZE = 4\n",
    "EMBED_DIM = 384\n",
    "NUM_HEADS = 6\n",
    "NUM_LAYERS = 8\n",
    "MLP_RATIO = 4.0\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 0.03\n",
    "SAVE_DIR = \"results\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[INFO] Device Type: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    # Print actual GPU name (ex: \"AMD Instinct MI300X\")\n",
    "    real_gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"[INFO] GPU Model: {real_gpu_name}\")\n",
    "\n",
    "# ============================================\n",
    "# 1) CIFAR-10 Dataloaders (RandAugment)\n",
    "# ============================================\n",
    "def get_cifar10_loaders(batch_size=128):\n",
    "    transform_train = Compose([\n",
    "        RandomCrop(32, padding=4),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandAugment(num_ops=2, magnitude=9),  # for better accuracy\n",
    "        ToTensor(),\n",
    "        Normalize([0.4914,0.4822,0.4465],[0.2470,0.2435,0.2616])\n",
    "    ])\n",
    "    transform_test = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize([0.4914,0.4822,0.4465],[0.2470,0.2435,0.2616])\n",
    "    ])\n",
    "    train_set = CIFAR10(root=\"data\", train=True, download=True, transform=transform_train)\n",
    "    test_set  = CIFAR10(root=\"data\", train=False, download=True, transform=transform_test)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ============================================\n",
    "# 2) Vision Transformer Building Blocks\n",
    "# ============================================\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=384):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.num_patches = (img_size // patch_size)**2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)        # [B, embed_dim, H', W']\n",
    "        x = x.flatten(2)        # [B, embed_dim, N]\n",
    "        x = x.transpose(1, 2)   # [B, N, embed_dim]\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim  = embed_dim // num_heads\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        # [B, N, 3*embed_dim] -> [B, N, 3, heads, head_dim] -> rearrange\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: [B, heads, N, head_dim]\n",
    "\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * (self.head_dim**-0.5)\n",
    "        attn = attn_scores.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop= nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn  = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp   = MLP(embed_dim, int(embed_dim*mlp_ratio), embed_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=32,\n",
    "        patch_size=4,\n",
    "        in_channels=3,\n",
    "        num_classes=10,\n",
    "        embed_dim=384,\n",
    "        depth=8,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches+1, embed_dim))\n",
    "        self.pos_drop  = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        self.apply(self._init_weights_general)\n",
    "\n",
    "    def _init_weights_general(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.trunc_normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)       # => [B, N, E]\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls, x), dim=1)# => [B, N+1, E]\n",
    "\n",
    "        x = x + self.pos_embed[:, :x.size(1), :]\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "        return logits\n",
    "\n",
    "# ============================================\n",
    "# 3) Train/Evaluate with Standard AMP\n",
    "# ============================================\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler, epoch, total_epochs):\n",
    "    model.train()\n",
    "    correct, total, running_loss = 0, 0, 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    for inputs, targets in pbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Use standard AMP (no forced dtype)\n",
    "        with autocast(enabled=True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * targets.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        acc = 100.0 * correct / total\n",
    "        pbar.set_postfix({\"loss\": f\"{running_loss/total:.4f}\", \"acc\": f\"{acc:.2f}\"})\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    return {\n",
    "        \"loss\": running_loss/total,\n",
    "        \"accuracy\": acc,\n",
    "        \"epoch_time\": epoch_time,\n",
    "        \"images_per_sec\": total/epoch_time\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct, total, running_loss = 0, 0, 0.0\n",
    "\n",
    "    for inputs, targets in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        with autocast(enabled=True):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "        running_loss += loss.item() * targets.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    return {\n",
    "        \"loss\": running_loss/total,\n",
    "        \"accuracy\": 100.0 * correct/total\n",
    "    }\n",
    "\n",
    "def visualize_predictions(model, loader, device, num_images=5):\n",
    "    \"\"\"\n",
    "    Plots a few random images from the test set with bigger figure.\n",
    "    Moves the images down so the title isn't merged with them.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_iter = iter(loader)\n",
    "    images, labels = next(data_iter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    with torch.no_grad(), autocast(enabled=True):\n",
    "        outputs = model(images)\n",
    "        _, preds = outputs.max(1)\n",
    "\n",
    "    class_names = loader.dataset.classes\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n",
    "    for i in range(num_images):\n",
    "        ax = axes[i]\n",
    "        idx = random.randint(0, len(images) - 1)\n",
    "        img = images[idx].cpu().numpy().transpose(1,2,0)\n",
    "        # Unnormalize\n",
    "        mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "        std  = np.array([0.2470, 0.2435, 0.2616])\n",
    "        img  = img*std + mean\n",
    "        img  = np.clip(img, 0, 1)\n",
    "\n",
    "        true_lbl = class_names[labels[idx].item()]\n",
    "        pred_lbl = class_names[preds[idx].item()]\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"True: {true_lbl}\\nPred: {pred_lbl}\", fontsize=11)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_anchor(\"C\")\n",
    "\n",
    "    \n",
    "    # Adjust the figure so there's enough space under the title\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "\n",
    "    # Or you can keep tight_layout() then re-adjust\n",
    "    # plt.tight_layout()\n",
    "    # plt.subplots_adjust(top=0.80)\n",
    "\n",
    "    out_png = os.path.join(SAVE_DIR, \"sample_predictions.png\")\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_inference(model, device, max_bs=128):\n",
    "    \"\"\"\n",
    "    Benchmarks inference for [1,4,16,64,128], saves to 'inference_stats.json'.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    for bs in [1,4,16,64,128]:\n",
    "        if bs > max_bs:\n",
    "            break\n",
    "        # We'll feed float32 inputs; autocast handles them\n",
    "        dummy = torch.randn(bs, 3, IMAGE_SIZE, IMAGE_SIZE, device=device)\n",
    "\n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            with autocast(enabled=True):\n",
    "                _ = model(dummy)\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        # Benchmark\n",
    "        start_t = time.time()\n",
    "        iters = 50 if bs <= 16 else 20\n",
    "        for _ in range(iters):\n",
    "            with autocast(enabled=True):\n",
    "                _ = model(dummy)\n",
    "        torch.cuda.synchronize()\n",
    "        dur = (time.time() - start_t)/iters\n",
    "\n",
    "        results[str(bs)] = {\n",
    "            \"inference_time_ms\": dur*1e3,\n",
    "            \"images_per_sec\": bs/dur\n",
    "        }\n",
    "        print(f\"[Inference] BS={bs}, {dur*1e3:.2f} ms, {bs/dur:.2f} img/s\")\n",
    "\n",
    "    out_path = os.path.join(SAVE_DIR,\"inference_stats.json\")\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(\"[INFO] Inference stats saved:\", out_path)\n",
    "\n",
    "def plot_training_metrics(train_stats, test_stats):\n",
    "    \"\"\"\n",
    "    Plots train/test loss & accuracy, with an MI300X mention in the title.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_stats)+1)\n",
    "    tr_loss= [s['loss'] for s in train_stats]\n",
    "    tr_acc = [s['accuracy'] for s in train_stats]\n",
    "    te_loss= [s['loss'] for s in test_stats]\n",
    "    te_acc = [s['accuracy'] for s in test_stats]\n",
    "\n",
    "    fig, axs = plt.subplots(1,2, figsize=(14,5))\n",
    "    fig.suptitle(\"Training on AMD Instinct MI300X (50 Epochs)\", fontsize=14)\n",
    "\n",
    "    # Loss\n",
    "    axs[0].plot(epochs, tr_loss, 'b-o', label='Train Loss')\n",
    "    axs[0].plot(epochs, te_loss, 'r-s', label='Test Loss')\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Accuracy\n",
    "    axs[1].plot(epochs, tr_acc, 'b-o', label='Train Acc')\n",
    "    axs[1].plot(epochs, te_acc, 'r-s', label='Test Acc')\n",
    "    axs[1].set_title(\"Accuracy (%)\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_fig = os.path.join(SAVE_DIR,\"training_metrics.png\")\n",
    "    plt.savefig(out_fig, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    print(\"[MAIN] Vision Transformer on CIFAR-10 (50 epochs, standard AMP)\")\n",
    "    train_loader, test_loader = get_cifar10_loaders(batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = VisionTransformer(\n",
    "        img_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        in_channels=3,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        depth=NUM_LAYERS,\n",
    "        num_heads=NUM_HEADS,\n",
    "        mlp_ratio=MLP_RATIO,\n",
    "        dropout=DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"[MAIN] #parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "    scaler    = GradScaler()\n",
    "\n",
    "    train_stats= []\n",
    "    test_stats = []\n",
    "    best_acc   = 0.0\n",
    "\n",
    "    start_all = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Train\n",
    "        tr_stat = train_one_epoch(model, train_loader, criterion, optimizer, device, scaler, epoch, NUM_EPOCHS)\n",
    "        train_stats.append(tr_stat)\n",
    "\n",
    "        # Evaluate\n",
    "        ev_stat = evaluate(model, test_loader, criterion, device)\n",
    "        test_stats.append(ev_stat)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{NUM_EPOCHS}] Train Acc={tr_stat['accuracy']:.2f}% | \"\n",
    "              f\"Test Acc={ev_stat['accuracy']:.2f}%\")\n",
    "        if ev_stat['accuracy']> best_acc:\n",
    "            best_acc = ev_stat['accuracy']\n",
    "        if ev_stat['accuracy']>= 90.0:\n",
    "            print(f\"[MAIN] Reached 90% test accuracy at epoch {epoch+1}, stopping early.\")\n",
    "            break\n",
    "\n",
    "    total_time = time.time() - start_all\n",
    "    print(f\"[MAIN] Training took {total_time/60:.2f} minutes. Best test acc={best_acc:.2f}%\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"checkpoints/vit_final_mi300x.pth\")\n",
    "    print(\"[MAIN] Model checkpoint saved.\")\n",
    "\n",
    "    # Plot training\n",
    "    plot_training_metrics(train_stats, test_stats)\n",
    "\n",
    "    # Visualize sample predictions\n",
    "    visualize_predictions(model, test_loader, device, num_images=5)\n",
    "\n",
    "    # Benchmark inference\n",
    "    benchmark_inference(model, device, max_bs=128)\n",
    "\n",
    "    print(\"[MAIN] All tasks done. Check 'results/' for plots and 'checkpoints/' for model.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36a523a3-d6d2-4cda-a255-6ded93491d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Analysis] performance_report.json not found.\n",
      "[Analysis] Saved inference scaling plot to analysis/mi300x_throughput_scaling.png\n",
      "[Analysis] Done. Check 'analysis/' folder for output plots.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MI300X Performance Analysis Script (No GPU Metrics)\n",
    "\n",
    "- Loads performance_report.json (if present) for optional reference\n",
    "- Loads inference_stats.json and creates a scaling efficiency plot\n",
    "- Saves output plots to 'analysis/' directory\n",
    "\n",
    "Usage:\n",
    "  python analysis_mi300x.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an 'analysis' folder if it doesn't exist\n",
    "os.makedirs(\"analysis\", exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads optional performance report from 'results/performance_report.json'\n",
    "    and inference stats from 'results/inference_stats.json'.\n",
    "    Returns a dict with keys:\n",
    "      'report' (dict or None)\n",
    "      'inference' (dict or None)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # 1) Attempt to load performance report\n",
    "    report_path = 'results/performance_report.json'\n",
    "    if os.path.isfile(report_path):\n",
    "        try:\n",
    "            with open(report_path, 'r') as f:\n",
    "                results['report'] = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[Analysis] Could not load performance_report.json: {e}\")\n",
    "            results['report'] = None\n",
    "    else:\n",
    "        print(\"[Analysis] performance_report.json not found.\")\n",
    "        results['report'] = None\n",
    "\n",
    "    # 2) Attempt to load inference stats\n",
    "    inference_path = 'results/inference_stats.json'\n",
    "    if os.path.isfile(inference_path):\n",
    "        try:\n",
    "            with open(inference_path, 'r') as f:\n",
    "                results['inference'] = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"[Analysis] Could not load inference_stats.json: {e}\")\n",
    "            results['inference'] = None\n",
    "    else:\n",
    "        print(\"[Analysis] inference_stats.json not found.\")\n",
    "        results['inference'] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_scaling_efficiency_plot(inference_data):\n",
    "    \"\"\"\n",
    "    Creates a scaling efficiency plot based on 'inference_data',\n",
    "    which should be a dict like:\n",
    "      {\n",
    "        \"1\": {\"images_per_sec\":..., \"inference_time_ms\":...},\n",
    "        \"4\": {...},\n",
    "        ...\n",
    "      }\n",
    "    Saves a 2-subplot figure to 'analysis/mi300x_throughput_scaling.png'.\n",
    "    \"\"\"\n",
    "    if not inference_data:\n",
    "        print(\"[Analysis] No inference data found; skipping scaling plot.\")\n",
    "        return\n",
    "\n",
    "    # Convert to a DataFrame\n",
    "    entries = []\n",
    "    for bs_str, stats in inference_data.items():\n",
    "        bs = int(bs_str)\n",
    "        entries.append({\n",
    "            'batch_size': bs,\n",
    "            'throughput': stats['images_per_sec'],\n",
    "            'latency_ms': stats['inference_time_ms']\n",
    "        })\n",
    "    inf_df = pd.DataFrame(entries).sort_values('batch_size')\n",
    "\n",
    "    # If batch_size=1 is present, compute ideal throughput & scaling efficiency\n",
    "    if (inf_df['batch_size'] == 1).any():\n",
    "        base_throughput = inf_df.loc[inf_df['batch_size'] == 1, 'throughput'].values[0]\n",
    "        inf_df['ideal_throughput'] = inf_df['batch_size'] * base_throughput\n",
    "        inf_df['scaling_efficiency'] = (inf_df['throughput'] / inf_df['ideal_throughput']) * 100\n",
    "    else:\n",
    "        inf_df['ideal_throughput'] = None\n",
    "        inf_df['scaling_efficiency'] = None\n",
    "        print(\"[Analysis] No batch_size=1 in data -> skipping ideal scaling lines.\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle(\"MI300X Inference Scaling Efficiency\", fontsize=14)\n",
    "\n",
    "    # Left subplot: actual throughput vs. batch size\n",
    "    ax1.plot(inf_df['batch_size'], inf_df['throughput'], 'bo-', label='Actual Throughput')\n",
    "    if inf_df['ideal_throughput'].notna().any():\n",
    "        ax1.plot(inf_df['batch_size'], inf_df['ideal_throughput'], 'r--', label='Ideal Linear')\n",
    "    ax1.set_xscale('log', base=2)\n",
    "    ax1.set_xlabel('Batch Size (log scale)')\n",
    "    ax1.set_ylabel('Throughput (images/sec)')\n",
    "    ax1.set_title('Throughput vs. Batch Size')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xticks(inf_df['batch_size'])\n",
    "    ax1.set_xticklabels([str(x) for x in inf_df['batch_size']])\n",
    "\n",
    "    # Right subplot: scaling efficiency\n",
    "    if inf_df['scaling_efficiency'].notna().any():\n",
    "        ax2.plot(inf_df['batch_size'], inf_df['scaling_efficiency'], 'go-')\n",
    "        ax2.set_xscale('log', base=2)\n",
    "        ax2.set_xlabel('Batch Size (log scale)')\n",
    "        ax2.set_ylabel('Scaling Efficiency (%)')\n",
    "        ax2.set_title('Scaling Efficiency')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_xticks(inf_df['batch_size'])\n",
    "        ax2.set_xticklabels([str(x) for x in inf_df['batch_size']])\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, \"No scaling efficiency (missing batch_size=1).\",\n",
    "                 ha='center', va='center', fontsize=12)\n",
    "        ax2.set_title(\"Scaling Efficiency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    out_path = 'analysis/mi300x_throughput_scaling.png'\n",
    "    plt.savefig(out_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[Analysis] Saved inference scaling plot to {out_path}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for analysis. Loads the data, then plots the inference scaling.\n",
    "    \"\"\"\n",
    "    data = load_data()\n",
    "\n",
    "    # If you have a performance report, you could parse or print it here\n",
    "    if data['report']:\n",
    "        print(\"[Analysis] Found a performance report. You can parse or print it below:\")\n",
    "        # For example:\n",
    "        # print(json.dumps(data['report'], indent=2))\n",
    "\n",
    "    # Create scaling efficiency plot from inference stats\n",
    "    create_scaling_efficiency_plot(data['inference'])\n",
    "\n",
    "    print(\"[Analysis] Done. Check 'analysis/' folder for output plots.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
